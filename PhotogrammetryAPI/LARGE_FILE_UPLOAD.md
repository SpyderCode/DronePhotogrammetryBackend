# ðŸ“¦ Large File Upload Support

## Overview

The API now supports uploading very large ZIP files containing thousands of images for photogrammetry processing.

## Upload Limits

### Current Configuration
- **Maximum Upload Size**: 50GB per file
- **Timeout**: 30 minutes for upload
- **Supported Format**: ZIP only
- **Image Formats**: JPG, JPEG, PNG
- **Recommended**: Hundreds to thousands of images

### How It Works

1. **Streaming Upload**: Files are streamed directly to disk, not buffered in memory
2. **No Memory Overhead**: Memory usage remains constant regardless of file size
3. **Progress Tracking**: Monitor upload via standard HTTP progress
4. **Automatic Handling**: ZIP extraction handles nested folders automatically

## Configuration

### Server Settings (`Program.cs`)

```csharp
// Kestrel limits
options.Limits.MaxRequestBodySize = 50L * 1024 * 1024 * 1024; // 50GB
options.Limits.RequestHeadersTimeout = TimeSpan.FromMinutes(30);
options.Limits.KeepAliveTimeout = TimeSpan.FromMinutes(30);

// Form options
options.MultipartBodyLengthLimit = 50L * 1024 * 1024 * 1024; // 50GB
```

### Controller Attributes

```csharp
[RequestSizeLimit(53687091200)] // 50GB
[RequestFormLimits(MultipartBodyLengthLimit = 53687091200)]
[DisableRequestSizeLimit]
```

## Upload Examples

### Using cURL (with progress)

```bash
TOKEN="your_jwt_token_here"

# Upload large ZIP file
curl -X POST http://localhost:5273/api/projects/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "ProjectName=Large Building Dataset" \
  -F "ZipFile=@large_images.zip" \
  --progress-bar \
  -o upload_response.json

# With timeout adjustment for very large files
curl -X POST http://localhost:5273/api/projects/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "ProjectName=Massive Dataset" \
  -F "ZipFile=@huge_images.zip" \
  --max-time 1800 \
  --progress-bar
```

### Using Python (with progress bar)

```python
import requests
from tqdm import tqdm
import os

BASE_URL = "http://localhost:5273"
TOKEN = "your_jwt_token_here"

def upload_large_file(zip_path, project_name):
    """Upload large ZIP file with progress bar"""
    
    file_size = os.path.getsize(zip_path)
    
    with open(zip_path, 'rb') as f:
        # Wrap file in tqdm for progress bar
        with tqdm(total=file_size, unit='B', unit_scale=True, desc='Uploading') as pbar:
            def read_callback(data):
                pbar.update(len(data))
                return data
            
            # Create multipart upload
            files = {
                'ZipFile': (os.path.basename(zip_path), f, 'application/zip')
            }
            data = {
                'ProjectName': project_name
            }
            headers = {
                'Authorization': f'Bearer {TOKEN}'
            }
            
            response = requests.post(
                f'{BASE_URL}/api/projects/upload',
                headers=headers,
                data=data,
                files=files,
                timeout=1800  # 30 minute timeout
            )
    
    return response.json()

# Usage
result = upload_large_file('dataset.zip', 'My Large Project')
print(f"Project ID: {result['projectId']}")
```

### Using C# Client

```csharp
using System.Net.Http;
using System.Net.Http.Headers;

public async Task<int> UploadLargeZipAsync(string zipPath, string projectName, string token)
{
    using var client = new HttpClient
    {
        Timeout = TimeSpan.FromMinutes(30)
    };
    
    using var form = new MultipartFormDataContent();
    
    // Add project name
    form.Add(new StringContent(projectName), "ProjectName");
    
    // Add file with streaming
    using var fileStream = File.OpenRead(zipPath);
    var fileContent = new StreamContent(fileStream);
    fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/zip");
    form.Add(fileContent, "ZipFile", Path.GetFileName(zipPath));
    
    // Add auth header
    client.DefaultRequestHeaders.Authorization = 
        new AuthenticationHeaderValue("Bearer", token);
    
    // Upload
    var response = await client.PostAsync(
        "http://localhost:5273/api/projects/upload", 
        form
    );
    
    response.EnsureSuccessStatusCode();
    
    var result = await response.Content.ReadFromJsonAsync<UploadResponse>();
    return result.ProjectId;
}
```

## Performance Guidelines

### Upload Times (Estimates)

| File Size | Local Network | WiFi | Upload Time |
|-----------|--------------|------|-------------|
| 1GB | ~10-20 sec | ~2-5 min | Instant |
| 5GB | ~1 min | ~10-20 min | Fast |
| 10GB | ~2 min | ~20-40 min | Quick |
| 25GB | ~5 min | ~50-90 min | Moderate |
| 50GB | ~10 min | ~2-3 hours | Manageable |

*Times for local deployment. Network speeds vary.*

### Processing Times After Upload

| Image Count | COLMAP Time (GPU) | Disk Space Used |
|-------------|-------------------|-----------------|
| 50 images | ~20-30 min | ~5GB |
| 100 images | ~45-60 min | ~10GB |
| 500 images | ~2-4 hours | ~50GB |
| 1000 images | ~4-8 hours | ~100GB |
| 5000 images | ~1-2 days | ~500GB |

*Times assume GPU acceleration and good image overlap*

## Disk Space Requirements

### Formula
```
Required Space = (ZIP Size) + (ZIP Size Ã— 2) + (Output Size)

Where:
- ZIP Size: Original upload
- ZIP Size Ã— 2: Extracted images + flattened copies
- Output Size: ~0.5-2x image size (depending on quality)
```

### Example
- 10GB ZIP upload
- ~20GB extracted and processed
- ~10GB output models
- **Total: ~40GB per project**

### Recommendations
- **Minimum Free Space**: 5x ZIP size
- **Monitoring**: Set up disk space alerts
- **Cleanup**: Implement automatic old project deletion
- **Storage**: Use separate large volume for `uploads/` and `models/`

## Optimization Tips

### 1. Image Preparation
```bash
# Compress images before zipping (reduce upload time)
# Using ImageMagick
mogrify -quality 85 -resize 3000x3000\> *.jpg

# Create optimized ZIP
zip -9 images.zip *.jpg
```

### 2. Network Optimization
- Use wired connection for large uploads
- Upload during off-peak hours
- Consider upload resumption for very large files
- Monitor network congestion

### 3. Server Configuration

**Increase disk I/O performance**:
```bash
# Mount upload directory with optimized flags
mount -o noatime,nodiratime /dev/sdX /path/to/uploads
```

**Monitor disk usage**:
```bash
# Set up disk usage monitoring
df -h /path/to/uploads
df -h /path/to/models
```

## Troubleshooting

### Upload Timeout

**Problem**: Upload fails after several minutes

**Solution**:
```bash
# Increase timeout in client
curl --max-time 3600 ...  # 1 hour

# Or in Python
requests.post(..., timeout=3600)
```

### Out of Disk Space

**Problem**: Upload fails with 500 error

**Solution**:
```bash
# Check available space
df -h

# Clean old projects
rm -rf uploads/old_project_*
rm -rf models/project_*/
```

### Slow Upload Speed

**Problem**: Upload taking much longer than expected

**Solutions**:
1. Check network speed: `speedtest-cli`
2. Use wired connection instead of WiFi
3. Upload during off-peak hours
4. Check server load: `top`, `htop`
5. Verify no bandwidth throttling

### Memory Issues

**Problem**: Server runs out of memory during upload

**Solution**: The API uses streaming, so this shouldn't happen. If it does:
```bash
# Check memory usage
free -h

# Restart API if needed
systemctl restart photogrammetry-api

# Increase system swap if necessary
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

## Security Considerations

### File Validation
- Only ZIP files accepted
- ZIP bomb protection (implicit via timeout)
- Virus scanning recommended for production

### Quota Management
```csharp
// Add per-user quota in ProjectService
public async Task<bool> CheckUserQuotaAsync(int userId, long fileSize)
{
    var userProjects = await _context.Projects
        .Where(p => p.UserId == userId)
        .ToListAsync();
    
    long totalSize = userProjects.Sum(p => p.ZipFileSize);
    const long maxQuotaBytes = 100L * 1024 * 1024 * 1024; // 100GB per user
    
    return (totalSize + fileSize) <= maxQuotaBytes;
}
```

## Monitoring

### Track Upload Statistics

```sql
-- Average upload size
SELECT AVG(LENGTH(ZipFilePath)) as AvgSize FROM Projects;

-- Largest uploads
SELECT Id, Name, ZipFilePath, CreatedAt 
FROM Projects 
ORDER BY LENGTH(ZipFilePath) DESC 
LIMIT 10;

-- Storage usage per user
SELECT UserId, COUNT(*) as Projects, SUM(LENGTH(ZipFilePath)) as TotalStorage
FROM Projects
GROUP BY UserId
ORDER BY TotalStorage DESC;
```

### Health Checks

```bash
# Disk usage alert
if [ $(df /path/to/uploads | tail -1 | awk '{print $5}' | sed 's/%//') -gt 80 ]; then
    echo "WARNING: Upload disk usage above 80%"
fi

# Monitor active uploads
netstat -an | grep :5273 | grep ESTABLISHED | wc -l
```

## Best Practices

1. **Compress images before zipping** - Reduces upload time
2. **Use high-quality images** - Better results worth the size
3. **Batch similar projects** - Group related image sets
4. **Clean up regularly** - Remove old/completed projects
5. **Monitor disk space** - Set up alerts at 70% usage
6. **Use resumable uploads** - For connections that may drop
7. **Validate locally first** - Test with small subset before full upload
8. **Document image metadata** - Include capture info in project name

## Future Enhancements

- [ ] Resumable uploads (chunked upload support)
- [ ] Progress webhooks for upload status
- [ ] Automatic compression if images uncompressed
- [ ] Multi-part upload for >50GB files
- [ ] S3/cloud storage integration
- [ ] Upload queue management
- [ ] Automatic old file cleanup
- [ ] Per-user storage quotas in database

---

**Status**: âœ… Large file uploads up to 50GB fully supported!

Test with your large datasets - the system is ready to handle thousands of images.
